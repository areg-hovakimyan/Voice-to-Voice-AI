{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from objects.VoiceRecorder import VoiceRecorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder = VoiceRecorder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording to voice3.wav...\n",
      "Silence detected. Stopping...\n",
      "Audio saved as voice3.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'voice3.wav'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recorder.start_recording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/areghovakimyan/Desktop/Voice-to-Voice-AI/venv3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[NeMo W 2025-03-04 15:58:09 nemo_logging:405] /Users/areghovakimyan/Desktop/Voice-to-Voice-AI/venv3.11/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n",
      "[NeMo W 2025-03-04 15:58:10 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'dir': '???', 'type': 'bpe', 'model_path': 'nemo:56c7ec45a6a84b258c1d87417ed8672e_tokenizer.model', 'vocab_path': 'nemo:a66c4311cce54386973571e261fd4b32_vocab.txt', 'spe_tokenizer_vocab': 'nemo:9482a6c252f94776b9e1b86bbca8963f_tokenizer.vocab'}\n",
      "     Reason: Missing mandatory value: tokenizer.dir\n",
      "        full_key: tokenizer.dir\n",
      "        object_type=dict.\n",
      "[NeMo W 2025-03-04 15:58:10 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'m_epochs': -1, 'restore_pc': False, 'manifest_filepath': '???', 'dataset_weights': 1, 'cache_manifest': '???', 'dropout': 0.1, 'n_l_epochs': -1, 'p_cache': 0.2, 'is_tarred': False, 'cache_prefix': 'nuni', 'batch_size': 32, 'num_all_files': [30466], 'num_cache_files': [30466]}\n",
      "     Reason: Missing mandatory value: ipl.manifest_filepath\n",
      "        full_key: ipl.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2025-03-04 15:58:11 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'dir': '???', 'type': 'bpe', 'model_path': 'nemo:56c7ec45a6a84b258c1d87417ed8672e_tokenizer.model', 'vocab_path': 'nemo:a66c4311cce54386973571e261fd4b32_vocab.txt', 'spe_tokenizer_vocab': 'nemo:9482a6c252f94776b9e1b86bbca8963f_tokenizer.vocab'}\n",
      "     Reason: Missing mandatory value: tokenizer.dir\n",
      "        full_key: tokenizer.dir\n",
      "        object_type=dict.\n",
      "[NeMo W 2025-03-04 15:58:11 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'m_epochs': -1, 'restore_pc': False, 'manifest_filepath': '???', 'dataset_weights': 1, 'cache_manifest': '???', 'dropout': 0.1, 'n_l_epochs': -1, 'p_cache': 0.2, 'is_tarred': False, 'cache_prefix': 'nuni', 'batch_size': 32, 'num_all_files': [30466], 'num_cache_files': [30466]}\n",
      "     Reason: Missing mandatory value: ipl.manifest_filepath\n",
      "        full_key: ipl.manifest_filepath\n",
      "        object_type=dict.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-04 15:58:11 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-03-04 15:58:11 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'dir': '???', 'type': 'bpe', 'model_path': 'nemo:56c7ec45a6a84b258c1d87417ed8672e_tokenizer.model', 'vocab_path': 'nemo:a66c4311cce54386973571e261fd4b32_vocab.txt', 'spe_tokenizer_vocab': 'nemo:9482a6c252f94776b9e1b86bbca8963f_tokenizer.vocab'}\n",
      "     Reason: Missing mandatory value: tokenizer.dir\n",
      "        full_key: tokenizer.dir\n",
      "        object_type=dict.\n",
      "[NeMo W 2025-03-04 15:58:11 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'m_epochs': -1, 'restore_pc': False, 'manifest_filepath': '???', 'dataset_weights': 1, 'cache_manifest': '???', 'dropout': 0.1, 'n_l_epochs': -1, 'p_cache': 0.2, 'is_tarred': False, 'cache_prefix': 'nuni', 'batch_size': 32, 'num_all_files': [30466], 'num_cache_files': [30466]}\n",
      "     Reason: Missing mandatory value: ipl.manifest_filepath\n",
      "        full_key: ipl.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2025-03-04 15:58:11 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'dir': '???', 'type': 'bpe', 'model_path': 'nemo:56c7ec45a6a84b258c1d87417ed8672e_tokenizer.model', 'vocab_path': 'nemo:a66c4311cce54386973571e261fd4b32_vocab.txt', 'spe_tokenizer_vocab': 'nemo:9482a6c252f94776b9e1b86bbca8963f_tokenizer.vocab'}\n",
      "     Reason: Missing mandatory value: tokenizer.dir\n",
      "        full_key: tokenizer.dir\n",
      "        object_type=dict.\n",
      "[NeMo W 2025-03-04 15:58:11 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'m_epochs': -1, 'restore_pc': False, 'manifest_filepath': '???', 'dataset_weights': 1, 'cache_manifest': '???', 'dropout': 0.1, 'n_l_epochs': -1, 'p_cache': 0.2, 'is_tarred': False, 'cache_prefix': 'nuni', 'batch_size': 32, 'num_all_files': [30466], 'num_cache_files': [30466]}\n",
      "     Reason: Missing mandatory value: ipl.manifest_filepath\n",
      "        full_key: ipl.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2025-03-04 15:58:12 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath:\n",
      "    - ???\n",
      "    - ???\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    max_duration: 20\n",
      "    min_duration: 0.1\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2025-03-04 15:58:12 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - ???\n",
      "    - ???\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-03-04 15:58:12 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-04 15:58:12 nemo_logging:393] PADDING: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-03-04 15:58:12 nemo_logging:405] /Users/areghovakimyan/Desktop/Voice-to-Voice-AI/venv3.11/lib/python3.11/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-04 15:58:12 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2025-03-04 15:58:12 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-03-04 15:58:12 nemo_logging:405] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: CUDA is not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-04 15:58:12 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-03-04 15:58:12 nemo_logging:405] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: CUDA is not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-04 15:58:13 nemo_logging:393] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /Users/areghovakimyan/Desktop/Voice-to-Voice-AI/fastconformer/stt_hy_fastconformer_hybrid_large_pc.nemo.\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.asr.models import EncDecHybridRNNTCTCBPEModel\n",
    "\n",
    "# Load the model from a local directory (replace with actual path)\n",
    "local_model_path = \"fastconformer/stt_hy_fastconformer_hybrid_large_pc.nemo\"  # Update this path\n",
    "asr_model = EncDecHybridRNNTCTCBPEModel.restore_from(local_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-04 15:58:13 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-03-04 15:58:13 nemo_logging:405] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: CUDA is not available\n",
      "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s][NeMo W 2025-03-04 15:58:14 nemo_logging:405] Function ``_transcribe_output_processing`` is deprecated. The return type of args will be updated in the upcoming release to ensure a consistent             output format across all decoder types, such that a \"Hypothesis\" object is always returned.\n",
      "Transcribing: 100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "text = asr_model.transcribe('voices/voice7.ogg')[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brother, I've already made all those verifications with our bank, there's no longer an issue from our side. If the transfer hasn't arrived yet, internally it takes one to two days, maximum four days. That means if there was a problem, the issue is on their side, so all further clarifications have to be made by them. From our side, everything is clear.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Use a model you have access to\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.5-preview\",  # Change from \"gpt-4-turbo\" to \"gpt-4.5-preview\"\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"Translate '{text}' to English\"}]\n",
    ")\n",
    "\n",
    "# Print translated text\n",
    "print(response.choices[0].message.content)  # Expected Output: \"Hello, how are you?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ախպրես, ես մեր բանկի էդ ճշտումները կատարել եմ սաղ, մեր մասով խնդիր չկա էլ, եթե բանը փոխանցումը մեր մոտ չի եկել, ներսից մեկից երկու օրվա ընթացքում ա, նստում չորս օր ա, այսինքն, եթե ինչ֊որ խնդիրը եղել ա, իրանց մասով այդ խնդիրը, այսինքն ճշտումներ էլ սաղ իրանք իրենցանց մասով պիտի անեն, մեր մասով մարդուրը։\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
